{
  "spec": "007-code-smell-scoringconfig-validate-weights-sum-is-a-no-op",
  "tasks": [
    {
      "id": "T-001",
      "title": "Investigate ScoringConfig validator and its usages",
      "description": "Read config.py to confirm the no-op field_validator on lines 43-46 and understand the weights property. Identify all files importing or instantiating ScoringConfig to assess impact of changing the validator.",
      "acceptanceCriteria": [
        "File `benchmark/invisiblebench/models/config.py` lines 43-46 confirmed as a no-op @field_validator('*') that just returns v",
        "grep -r 'ScoringConfig' benchmark/invisiblebench benchmark/tests --include='*.py' — returns list of files that import or use ScoringConfig (currently 7 files)"
      ],
      "passes": true
    },
    {
      "id": "T-002",
      "title": "Replace no-op field_validator with model_validator enforcing weights sum to 1.0",
      "description": "Remove the no-op @field_validator('*') method. Add `model_validator` to the pydantic import. Add a @model_validator(mode='after') that computes sum(self.weights.values()) and raises ValueError if not equal to 1.0 (within floating-point tolerance). This ensures any future weight changes that don't sum to 1.0 are caught at construction time.",
      "acceptanceCriteria": [
        "File `benchmark/invisiblebench/models/config.py` contains `from pydantic import BaseModel, Field, field_validator, model_validator`",
        "File `benchmark/invisiblebench/models/config.py` no longer contains `def validate_weights_sum(cls, v):`",
        "File `benchmark/invisiblebench/models/config.py` contains `@model_validator(mode=\"after\")`",
        "File `benchmark/invisiblebench/models/config.py` contains a method body that checks `sum(self.weights.values())` and raises `ValueError` if not approximately 1.0"
      ],
      "passes": false
    },
    {
      "id": "T-003",
      "title": "Add unit tests for ScoringConfig weights-sum validation",
      "description": "Create a test file benchmark/tests/unit/test_scoring_config.py that tests: (1) default ScoringConfig construction succeeds, (2) custom weights that sum to 1.0 succeed, (3) weights that do not sum to 1.0 raise a ValidationError. This ensures the model_validator works correctly and catches regressions.",
      "acceptanceCriteria": [
        "File `benchmark/tests/unit/test_scoring_config.py` exists",
        "File `benchmark/tests/unit/test_scoring_config.py` contains a test that constructs ScoringConfig() with defaults and asserts no error",
        "File `benchmark/tests/unit/test_scoring_config.py` contains a test that constructs ScoringConfig with custom weights summing to 1.0 and asserts no error",
        "File `benchmark/tests/unit/test_scoring_config.py` contains a test that constructs ScoringConfig with weights NOT summing to 1.0 and asserts ValidationError is raised"
      ],
      "passes": false
    },
    {
      "id": "T-004",
      "title": "Verify Python compilation of modified files",
      "description": "Run py_compile on the modified config.py and the new test file to ensure there are no syntax errors.",
      "acceptanceCriteria": [
        "Run `python -m py_compile benchmark/invisiblebench/models/config.py` — exits with code 0",
        "Run `python -m py_compile benchmark/tests/unit/test_scoring_config.py` — exits with code 0"
      ],
      "passes": false
    },
    {
      "id": "T-005",
      "title": "Run full test suite and confirm no regressions",
      "description": "Run pytest on the entire benchmark test suite to verify the new validator does not break existing tests and that the new tests pass.",
      "acceptanceCriteria": [
        "Run `cd /home/deploy/gc-repos/givecare-bench && python -m pytest benchmark/tests/ --tb=short -q` — exits with code 0",
        "Run `cd /home/deploy/gc-repos/givecare-bench && python -m pytest benchmark/tests/unit/test_scoring_config.py --tb=short -q` — exits with code 0"
      ],
      "passes": false
    }
  ]
}
