[
  {
    "title": "Bug: Dead code / unreachable variable in orchestrator",
    "effort": "unknown",
    "slug": "bug-dead-code-unreachable-variable-in-orchestrator",
    "source": "ai-review-2026-01-27",
    "status": "actioned",
    "type": "improvement",
    "body": "`benchmark/invisiblebench/evaluation/orchestrator.py:423` — The variable `_single_iteration_mode = True` is assigned inside `score()` but is only used later at line 671. It's always `True` in the single-iteration path, so the conditional is meaningless. More importantly, if the multi-iteration path (`iterations > 1`) returns early at line 417, this variable never exists in that scope. This isn't buggy per se since the early return skips line 671, but the flag pattern is confusing — the single-iteration wrapping at lines 671-681 always runs."
  },
  {
    "title": "Bug: Duplicate model entries diverge from CLI numbering",
    "effort": "unknown",
    "slug": "bug-duplicate-model-entries-diverge-from-cli-numbering",
    "source": "ai-review-2026-01-27",
    "status": "actioned",
    "type": "improvement",
    "body": "`benchmark/invisiblebench/models/config.py:102-173` vs `benchmark/invisiblebench/cli/runner.py:64-65` — The `MODELS_FULL` list and the CLI's `MODELS_FULL` (derived via `model_dump()`) contain 10 entries, but the `DEFAULT_TEST_MODELS` list in `api/client.py:285-296` has completely different model IDs (e.g., `openai/gpt-5` vs `openai/gpt-5.2-20251211`, `meta-llama/llama-3.1-70b-instruct` vs `qwen/qwen3-235b-a22b`). These two model lists will drift silently. `DEFAULT_TEST_MODELS` appears unused — it should be removed or reconciled."
  },
  {
    "title": "Anti-pattern: Massive code duplication in `runner.py`",
    "effort": "unknown",
    "slug": "anti-pattern-massive-code-duplication-in-runner-py",
    "source": "ai-review-2026-01-27",
    "status": "actioned",
    "type": "improvement",
    "body": "`benchmark/invisiblebench/cli/runner.py` — The transcript generation + scoring + result-building logic is copy-pasted across 4 code paths:\n- `generate_transcript()` (sync, lines 348-404)\n- `evaluate_scenario_async()` (async, lines 435-591)\n- Rich live display path (lines 859-960)\n- Non-rich fallback path (lines 961-1043)\n\nEach duplicates the same `conversation_history` building, error result construction, and result summary dict. A single `evaluate_scenario()` function with sync/async dispatch would eliminate ~300 lines."
  },
  {
    "title": "Anti-pattern: `_collect_probes",
    "effort": "unknown",
    "slug": "anti-pattern-collect-probes",
    "source": "ai-review-2026-01-27",
    "status": "actioned",
    "type": "improvement",
    "body": "`benchmark/invisiblebench/evaluation/scorers/memory.py:16-25` and `benchmark/invisiblebench/evaluation/scorers/trauma.py:24-33` have identical `_collect_probes()` implementations. This should be a shared utility."
  },
  {
    "title": "Code smell: Scorer LLM response parsing is fragile",
    "effort": "unknown",
    "slug": "code-smell-scorer-llm-response-parsing-is-fragile",
    "source": "ai-review-2026-01-27",
    "status": "actioned",
    "type": "improvement",
    "body": "All scorers (`safety.py:418-459`, `trauma.py:532-570`, `belonging.py:309-372`, `compliance.py:398-469`) parse LLM responses by splitting on newlines and checking `startswith()`. A single malformed line (extra colon in the value, missing space) silently falls through to default values. There's no warning when parsing fails partially."
  },
  {
    "title": "Code smell: `ScoringConfig.validate_weights_sum` is a no-op",
    "effort": "unknown",
    "slug": "code-smell-scoringconfig-validate-weights-sum-is-a-no-op",
    "source": "ai-review-2026-01-27",
    "status": "actioned",
    "type": "improvement",
    "body": "`benchmark/invisiblebench/models/config.py:43-46` — The validator is decorated with `@field_validator(\"*\")` but the body just `return v`. The weights (0.15+0.20+0.15+0.25+0.25 = 1.0) happen to sum to 1.0 today, but nothing enforces it. A `model_validator` checking `sum(weights.values()) == 1.0` would catch regressions."
  },
  {
    "title": "Excessive LLM API calls in safety scorer",
    "effort": "unknown",
    "slug": "excessive-llm-api-calls-in-safety-scorer",
    "source": "ai-review-2026-01-27",
    "status": "actioned",
    "type": "improvement",
    "body": "`benchmark/invisiblebench/evaluation/scorers/safety.py:190-237` — Each safety evaluation makes **6 LLM API calls**: 1 reference generation + 5 judgment distribution samples. The compliance scorer adds 3 more, trauma adds 1-4 (boundary probes), belonging adds 1. That's **11+ API calls per scenario** for scoring alone, on top of the model-under-test calls. For a 29-scenario benchmark with 10 models, that's ~3,200 scorer API calls. Consider reducing `n_samples` to 3 for safety (matching compliance) or caching the reference response."
  },
  {
    "title": "`httpx.AsyncClient` created per-call in async path",
    "effort": "unknown",
    "slug": "httpx-asyncclient-created-per-call-in-async-path",
    "source": "ai-review-2026-01-27",
    "status": "actioned",
    "type": "improvement",
    "body": "`benchmark/invisiblebench/api/client.py:192-195` — Each `call_model_async()` invocation creates a new `AsyncClient` with its own connection pool. When running 29 scenarios per model, this means 29+ TLS handshakes. The client should be created once and reused (e.g., stored on `ModelAPIClient` and closed explicitly)."
  },
  {
    "title": "Orchestrator reloads files on every iteration",
    "effort": "unknown",
    "slug": "orchestrator-reloads-files-on-every-iteration",
    "source": "ai-review-2026-01-27",
    "status": "actioned",
    "type": "improvement",
    "body": "`benchmark/invisiblebench/evaluation/orchestrator.py:427-433` — When `_score_with_iterations` calls `self.score()` recursively for each iteration, each call re-loads the transcript, scenario, and rules from disk via the loaders. These files don't change between iterations — they should be loaded once and passed through."
  },
  {
    "title": "Mixed data representations: dicts vs. Pydantic models",
    "effort": "unknown",
    "slug": "mixed-data-representations-dicts-vs-pydantic-models",
    "source": "ai-review-2026-01-27",
    "status": "actioned",
    "type": "improvement",
    "body": "The codebase has two parallel model systems:\n- Pydantic models in `models/config.py`, `models/results.py`, `models/scenario.py`\n- Legacy dataclasses in `models/__init__.py` (lines 66-241)\n- Raw dicts throughout the scorers and orchestrator\n\nThe orchestrator, all scorers, and the CLI operate entirely on raw dicts (`Dict[str, Any]`). The Pydantic models (`ScenarioResult`, `EvalResult`, `BatchResult`) are defined but never instantiated in the actual benchmark pipeline. This means you get zero runtime validation from Pydantic in production."
  },
  {
    "title": "Tight coupling: `runner.py` reimplements orchestrator logic",
    "effort": "unknown",
    "slug": "tight-coupling-runner-py-reimplements-orchestrator-logic",
    "source": "ai-review-2026-01-27",
    "status": "actioned",
    "type": "improvement",
    "body": "The CLI runner doesn't use `ScoringOrchestrator.score()` for the full pipeline — it separately generates transcripts, then calls `orchestrator.score()` for scoring only. But it also duplicates result aggregation, cost tracking, and pass/fail logic. The runner should delegate more to the orchestrator rather than re-implementing result assembly."
  },
  {
    "title": "No scorer interface/protocol",
    "effort": "unknown",
    "slug": "no-scorer-interface-protocol",
    "source": "ai-review-2026-01-27",
    "status": "actioned",
    "type": "improvement",
    "body": "Each scorer module (`safety.py`, `compliance.py`, `trauma.py`, `belonging.py`, `memory.py`) exposes a `score()` function with different signatures. Safety and compliance take `rules`, trauma and belonging don't. There's no `Protocol` or base class enforcing the contract. The orchestrator knows each scorer's signature via individual lambda calls (`orchestrator.py:501-578`), making it impossible to add a new scorer without modifying the orchestrator."
  },
  {
    "title": "No test fixtures for LLM-enabled code paths",
    "effort": "unknown",
    "slug": "no-test-fixtures-for-llm-enabled-code-paths",
    "source": "ai-review-2026-01-27",
    "status": "actioned",
    "type": "improvement",
    "body": "`conftest.py:8` sets `INVISIBLEBENCH_DISABLE_LLM=1` globally. All tests run in offline/deterministic mode. The LLM-based scoring paths (which are the primary production paths with the judgment distribution, reference generation, and multi-sample voting) are completely untested. There are no mocked API client tests verifying that LLM response parsing produces correct scores."
  },
  {
    "title": "Missing edge case tests for scorer parsing",
    "effort": "unknown",
    "slug": "missing-edge-case-tests-for-scorer-parsing",
    "source": "ai-review-2026-01-27",
    "status": "actioned",
    "type": "improvement",
    "body": "The LLM response parsers in each scorer have no dedicated tests. Edge cases like:\n- LLM returns `RESPONSE_QUALITY: 0.8 (good response)` (extra text after number)\n- LLM returns `CRISIS_DETECTED: Yes` (capitalized) vs `yes`\n- LLM omits a required field entirely\n- LLM returns scores outside 0.0-1.0 range\n\nNone of these are tested."
  },
  {
    "title": "No integration test for the full pipeline",
    "effort": "unknown",
    "slug": "no-integration-test-for-the-full-pipeline",
    "source": "ai-review-2026-01-27",
    "status": "actioned",
    "type": "improvement",
    "body": "There's `benchmark/tests/integration/test_orchestrator.py` and `test_cli.py`, but no test that runs `generate_transcript` -> `orchestrator.score()` -> `ReportGenerator` end-to-end with a mocked API client. The actual data flow through the system is untested."
  }
]
