% Core methodology citations
@article{zhang2024train,
  title={Train Before Test: How to Aggregate Rankings in LLM Benchmarks},
  author={Zhang, Guanhua and others},
  journal={arXiv preprint},
  year={2024},
  note={Framework for as-deployed capability vs inherent potential; PCA methodology}
}

@article{he2025impatient,
  title={Impatient Users Confuse AI Agents: High-fidelity Simulations of Human Traits for Testing Agents},
  author={He, Muyu and Kumar, Anand and Mackey, Tsach and Rajeev, Meghana and Zou, James and Rajani, Nazneen},
  journal={arXiv preprint arXiv:2510.04491v1},
  year={2025},
  note={TraitBasis methodology for stress trait simulation}
}

@misc{huggingface2024yourbench,
  title={YourBench: Building Custom LLM Benchmarks for Your Application},
  author={{Hugging Face}},
  year={2024},
  howpublished={\url{https://huggingface.co/spaces/yourbench/advanced}},
  note={Custom benchmark methodology and contamination prevention}
}

% Caregiver statistics
@misc{aarp2025,
  title={Caregiving in America 2025},
  author={{AARP and National Alliance for Caregiving}},
  year={2025},
  howpublished={\url{https://www.aarp.org/caregiving}},
  note={Source for caregiver demographics and burden statistics}
}

% Existing benchmarks
@article{truthfulqa,
  title={TruthfulQA: Measuring How Models Mimic Human Falsehoods},
  author={Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  journal={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics},
  year={2022},
  note={Single-turn factual accuracy benchmark}
}

@article{harmbench,
  title={HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal},
  author={Mazeika, Mantas and others},
  journal={arXiv preprint},
  year={2024},
  note={Harmful content generation testing across 18 categories}
}

@article{safetybench,
  title={SafetyBench: Evaluating the Safety of Large Language Models},
  author={Zhang, Zhexin and others},
  journal={arXiv preprint},
  year={2023},
  note={Multi-dimensional safety benchmark, single-turn}
}

@misc{eqbench2024,
  title={EQ-Bench: An Emotional Intelligence Benchmark for Large Language Models},
  author={Paech, Samuel J.},
  year={2024},
  howpublished={\url{https://eqbench.com}},
  note={Emotional intelligence testing, maximum 3 turns}
}

@article{winslow2025sharp,
  title={A Principle-based Framework for the Development and Evaluation of Large Language Models for Health and Wellness},
  author={Winslow, Brennan and Shreibati, Jake and Perez, Juan and Su, Hao-Wei and Young-Lin, Nicole and Hammerquist, Natalie and McDuff, Daniel and Guss, Jonathan and Vafeiadou, Julianna and Cain, Nicholas and Lin, Angela and Schenck, Eric and Rajagopal, Sandhya and Chung, Jen-Ruei and Venkatakrishnan, Ashwin and Lee, Albert A. and Karimzadehgan, Maryam and Meng, Qingze and Agarwal, Rishabh and Natarajan, Arjun and Giest, Tom},
  journal={Google Research},
  year={2025},
  note={SHARP Framework: Safety (adversarial, harm), Helpfulness (value, actionability), Accuracy (factuality, consensus), Relevance (grounding, comprehensiveness), Personalization (tone, fairness, health literacy). Validated on Fitbit Insights explorer system (13,300 users, 5-month staged deployment). Evaluation infrastructure: 18 generalist + 15 clinical specialist raters with interactive training}
}

@misc{rosebud2024,
  title={CARE: Crisis Assessment and Response Evaluation Benchmark},
  author={{Rosebud AI}},
  year={2024},
  howpublished={\url{https://rosebud.ai/care}},
  note={Crisis detection in single mental health messages}
}

@article{medqa,
  title={What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams},
  author={Jin, Di and others},
  journal={Applied Sciences},
  volume={11},
  number={14},
  year={2021},
  note={Medical question-answering benchmark}
}

% Long-context and performance degradation
@article{liu2023lost,
  title={Lost in the Middle: How Language Models Use Long Contexts},
  author={Liu, Nelson F. and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
  journal={Transactions of the Association for Computational Linguistics},
  year={2023},
  note={39\% accuracy decline in long-context retrieval}
}

@article{helmet2024,
  title={HELMET: How to Evaluate Long-Context Language Models Effectively and Thoroughly},
  author={Abhay and others},
  journal={arXiv preprint},
  year={2024},
  note={Multi-turn capability evaluation (not safety-focused)}
}

% Failure mode research
@misc{replika2024,
  title={Replika: The AI Companion Who Cares},
  author={{Replika Inc.}},
  year={2024},
  howpublished={\url{https://replika.com}},
  note={Case study: parasocial attachment in AI companions}
}

@article{berkeley2024,
  title={Othering and Belonging: Expanding the Circle of Human Concern},
  author={Powell, John A. and Menendian, Stephen and Ake, Wendy},
  journal={Haas Institute for a Fair and Inclusive Society, UC Berkeley},
  year={2024},
  note={Framework for cultural othering and bias patterns}
}

@misc{stanford2024,
  title={Bridge Crisis Counseling: AI Risk Assessment Study},
  author={{Stanford HAI}},
  year={2024},
  howpublished={\url{https://hai.stanford.edu}},
  note={86\% missed masked suicidal ideation rate}
}


% New caregiver-specific AI research (2025)
@article{shi2025carey,
  author={Shi, Jiayue Melissa and Yoo, Dong Whi and Wang, Keran and Rodriguez, Violeta J. and Karkar, Ravi and Saha, Koustuv},
  title={Mapping Caregiver Needs to AI Chatbot Design},
  journal={arXiv preprint arXiv:2506.15047},
  year={2025},
  note={Caregiver needs assessment for AI chatbot design}
}

@article{shi2025temporal,
  author={Shi, Jiayue Melissa and Wang, Keran and Yoo, Dong Whi and Karkar, Ravi and Saha, Koustuv},
  title={Balancing Caregiving and Self-Care: Exploring Mental Health Needs of Alzheimer's and Dementia Caregivers},
  journal={arXiv preprint arXiv:2506.14196},
  year={2025},
  note={Mental health needs of dementia caregivers}
}

@article{korpan2025bias,
  author={Korpan, Raj},
  title={Encoding Inequity: Examining Demographic Bias in LLM-Driven Robot Caregiving},
  journal={arXiv preprint arXiv:2503.05765},
  year={2025},
  note={Empirical evidence of demographic bias in caregiving AI - disability, age, LGBTQ+ stereotyping}
}

@article{kaur2025corus,
  author={Kaur, Navreet and Ayad, Hoda and Jung, Hayoung and Mittal, Shravika and De Choudhury, Munmun and Mitra, Tanushree},
  title={Who's Asking? Simulating Role-Based Questions for Conversational AI Evaluation},
  journal={arXiv preprint arXiv:2510.16829},
  year={2025},
  note={Role-based response asymmetry: vulnerable roles get 17\% more support but 19\% less knowledge}
}

@article{welivita2024empathy,
  author={Welivita, Anuradha and Pu, Pearl},
  title={Is ChatGPT More Empathetic than Humans?},
  journal={arXiv preprint arXiv:2403.05572},
  year={2024},
  note={Three-component empathy model: cognitive, affective, compassionate}
}

@article{xu2025mentalchat,
  title={MentalChat16K: A Benchmark Dataset for Conversational Mental Health Assistance},
  author={Xu, Jia and Wei, Tianyi and Hou, Bojian and Orzechowski, Patryk and Yang, Shu and Jin, Ruochen and Paulbeck, Rachael and Wagenaar, Joost and Demiris, George and Shen, Li},
  journal={arXiv preprint arXiv:2503.13509},
  year={2025}
}

@article{waaler2024schizophrenia,
  title={Prompt Engineering a Schizophrenia Chatbot: Utilizing a Multi-Agent Approach for Enhanced Compliance with Prompt Instructions},
  author={Waaler, Per Niklas and Hussain, Musarrat and Molchanov, Igor and Bongo, Lars Ailo and Elvev{\aa}g, Brita},
  journal={arXiv preprint arXiv:2410.12848},
  year={2024}
}

@article{kowal2025ape,
  title={It's the Thought that Counts: Evaluating the Attempts of Frontier LLMs to Persuade on Harmful Topics},
  author={Kowal, Matthew and Timm, Jasper and Godbout, Jean-Francois and Costello, Thomas and Arechar, Antonio A and Pennycook, Gordon and Rand, David and Gleave, Adam and Pelrine, Kellin},
  journal={arXiv preprint arXiv:2506.02873},
  year={2025}
}

@article{chiang2025therapy,
  title={Do We Talk to Robots Like Therapists, and Do They Respond Accordingly? Language Alignment in AI Emotional Support},
  author={Chiang, Sophie and Laban, Guy and Gunes, Hatice},
  journal={arXiv preprint arXiv:2506.16473},
  year={2025}
}

@article{guo2025hopebot,
  title={Development and Evaluation of HopeBot: an LLM-based chatbot for structured and interactive PHQ-9 depression screening},
  author={Guo, Zhijun and Lai, Alvina and Ive, Julia and Petcu, Alexandru and Wang, Yutong and Qi, Luyuan and Thygesen, Johan H and Li, Kezhi},
  journal={arXiv preprint arXiv:2507.05984},
  year={2025}
}

@article{kursuncu2025reddit,
  title={From Reddit to Generative AI: Evaluating Large Language Models for Anxiety Support Fine-tuned on Social Media Data},
  author={Kursuncu, Ugur and Padhi, Trilok and Sinha, Gaurav and Erol, Abdulkadir and Mandivarapu, Jaya Krishna and Larrison, Christopher R},
  journal={arXiv preprint arXiv:2505.18464},
  year={2025}
}

% Additional relevant AI safety research
@article{zhang2024empathy,
  author={Zhang, Yiren and others},
  title={Evaluating Empathy in AI Conversational Agents},
  journal={Proceedings of CHI 2024},
  year={2024},
  note={Framework for evaluating empathy components in conversational AI}
}

@article{morris2024longitudinal,
  author={Morris, Meredith Ringel and Suh, Jina and others},
  title={Safety Considerations for Long-Context Language Models},
  journal={arXiv preprint},
  year={2024},
  note={Long-context safety evaluation and failure modes}
}

@article{bender2024cultural,
  author={Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  title={On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
  journal={Proceedings of FAccT 2021},
  year={2021},
  note={Cultural bias and representational harms in large language models}
}

@article{liu2024memory,
  author={Liu, Alisa and Swayamdipta, Swabha and Smith, Noah A. and Choi, Yejin},
  title={WMDP: Measuring and Reducing Malicious Use With Unlearning},
  journal={arXiv preprint},
  year={2024},
  note={Memory management and privacy in AI systems}
}

% Updated regulatory framework
@legislation{illinois_wopr_2025,
  title={Wellness and Oversight for Psychological Resources (WOPR) Act},
  number={House Bill 1806 / Public Act 104-0054},
  jurisdiction={Illinois},
  year={2025},
  effective={August 1, 2025},
  url={https://ilga.gov/legislation/publicacts/104/104-0054.htm}
}

% Related work - GiveCare system
@misc{madad2025givecare,
  title={GiveCare: A Reference Architecture for Longitudinal-Safe Caregiving AI with SDOH Assessment and Multi-Agent Design},
  author={Madad, Ali},
  year={2025},
  eprint={2501.YYYYY},
  archivePrefix={arXiv},
  primaryClass={cs.AI},
  note={To be updated with actual arXiv number upon submission}
}

% This work
@misc{madad2025supportbench,
  title={SupportBench: A Benchmark for Evaluating AI Safety in Persistent Caregiving Relationships},
  author={Madad, Ali},
  year={2025},
  howpublished={\url{https://github.com/givecare/supportbench}},
  note={This work}
}
