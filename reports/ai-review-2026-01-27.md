I now have a thorough understanding of the codebase. Here is the review.

---

# Code Review: givecare-bench (InvisibleBench)

## Code Quality

### 1. Bug: Dead code / unreachable variable in orchestrator
`benchmark/invisiblebench/evaluation/orchestrator.py:423` — The variable `_single_iteration_mode = True` is assigned inside `score()` but is only used later at line 671. It's always `True` in the single-iteration path, so the conditional is meaningless. More importantly, if the multi-iteration path (`iterations > 1`) returns early at line 417, this variable never exists in that scope. This isn't buggy per se since the early return skips line 671, but the flag pattern is confusing — the single-iteration wrapping at lines 671-681 always runs.

### 2. Bug: Duplicate model entries diverge from CLI numbering
`benchmark/invisiblebench/models/config.py:102-173` vs `benchmark/invisiblebench/cli/runner.py:64-65` — The `MODELS_FULL` list and the CLI's `MODELS_FULL` (derived via `model_dump()`) contain 10 entries, but the `DEFAULT_TEST_MODELS` list in `api/client.py:285-296` has completely different model IDs (e.g., `openai/gpt-5` vs `openai/gpt-5.2-20251211`, `meta-llama/llama-3.1-70b-instruct` vs `qwen/qwen3-235b-a22b`). These two model lists will drift silently. `DEFAULT_TEST_MODELS` appears unused — it should be removed or reconciled.

### 3. Anti-pattern: Massive code duplication in `runner.py`
`benchmark/invisiblebench/cli/runner.py` — The transcript generation + scoring + result-building logic is copy-pasted across 4 code paths:
- `generate_transcript()` (sync, lines 348-404)
- `evaluate_scenario_async()` (async, lines 435-591)
- Rich live display path (lines 859-960)
- Non-rich fallback path (lines 961-1043)

Each duplicates the same `conversation_history` building, error result construction, and result summary dict. A single `evaluate_scenario()` function with sync/async dispatch would eliminate ~300 lines.

### 4. Anti-pattern: `_collect_probes()` duplicated across scorers
`benchmark/invisiblebench/evaluation/scorers/memory.py:16-25` and `benchmark/invisiblebench/evaluation/scorers/trauma.py:24-33` have identical `_collect_probes()` implementations. This should be a shared utility.

### 5. Code smell: Scorer LLM response parsing is fragile
All scorers (`safety.py:418-459`, `trauma.py:532-570`, `belonging.py:309-372`, `compliance.py:398-469`) parse LLM responses by splitting on newlines and checking `startswith()`. A single malformed line (extra colon in the value, missing space) silently falls through to default values. There's no warning when parsing fails partially.

### 6. Code smell: `ScoringConfig.validate_weights_sum` is a no-op
`benchmark/invisiblebench/models/config.py:43-46` — The validator is decorated with `@field_validator("*")` but the body just `return v`. The weights (0.15+0.20+0.15+0.25+0.25 = 1.0) happen to sum to 1.0 today, but nothing enforces it. A `model_validator` checking `sum(weights.values()) == 1.0` would catch regressions.

## Performance

### 7. Excessive LLM API calls in safety scorer
`benchmark/invisiblebench/evaluation/scorers/safety.py:190-237` — Each safety evaluation makes **6 LLM API calls**: 1 reference generation + 5 judgment distribution samples. The compliance scorer adds 3 more, trauma adds 1-4 (boundary probes), belonging adds 1. That's **11+ API calls per scenario** for scoring alone, on top of the model-under-test calls. For a 29-scenario benchmark with 10 models, that's ~3,200 scorer API calls. Consider reducing `n_samples` to 3 for safety (matching compliance) or caching the reference response.

### 8. `httpx.AsyncClient` created per-call in async path
`benchmark/invisiblebench/api/client.py:192-195` — Each `call_model_async()` invocation creates a new `AsyncClient` with its own connection pool. When running 29 scenarios per model, this means 29+ TLS handshakes. The client should be created once and reused (e.g., stored on `ModelAPIClient` and closed explicitly).

### 9. Orchestrator reloads files on every iteration
`benchmark/invisiblebench/evaluation/orchestrator.py:427-433` — When `_score_with_iterations` calls `self.score()` recursively for each iteration, each call re-loads the transcript, scenario, and rules from disk via the loaders. These files don't change between iterations — they should be loaded once and passed through.

## Architecture

### 10. Mixed data representations: dicts vs. Pydantic models
The codebase has two parallel model systems:
- Pydantic models in `models/config.py`, `models/results.py`, `models/scenario.py`
- Legacy dataclasses in `models/__init__.py` (lines 66-241)
- Raw dicts throughout the scorers and orchestrator

The orchestrator, all scorers, and the CLI operate entirely on raw dicts (`Dict[str, Any]`). The Pydantic models (`ScenarioResult`, `EvalResult`, `BatchResult`) are defined but never instantiated in the actual benchmark pipeline. This means you get zero runtime validation from Pydantic in production.

### 11. Tight coupling: `runner.py` reimplements orchestrator logic
The CLI runner doesn't use `ScoringOrchestrator.score()` for the full pipeline — it separately generates transcripts, then calls `orchestrator.score()` for scoring only. But it also duplicates result aggregation, cost tracking, and pass/fail logic. The runner should delegate more to the orchestrator rather than re-implementing result assembly.

### 12. No scorer interface/protocol
Each scorer module (`safety.py`, `compliance.py`, `trauma.py`, `belonging.py`, `memory.py`) exposes a `score()` function with different signatures. Safety and compliance take `rules`, trauma and belonging don't. There's no `Protocol` or base class enforcing the contract. The orchestrator knows each scorer's signature via individual lambda calls (`orchestrator.py:501-578`), making it impossible to add a new scorer without modifying the orchestrator.

## Testing

### 13. No test fixtures for LLM-enabled code paths
`conftest.py:8` sets `INVISIBLEBENCH_DISABLE_LLM=1` globally. All tests run in offline/deterministic mode. The LLM-based scoring paths (which are the primary production paths with the judgment distribution, reference generation, and multi-sample voting) are completely untested. There are no mocked API client tests verifying that LLM response parsing produces correct scores.

### 14. Missing edge case tests for scorer parsing
The LLM response parsers in each scorer have no dedicated tests. Edge cases like:
- LLM returns `RESPONSE_QUALITY: 0.8 (good response)` (extra text after number)
- LLM returns `CRISIS_DETECTED: Yes` (capitalized) vs `yes`
- LLM omits a required field entirely
- LLM returns scores outside 0.0-1.0 range

None of these are tested.

### 15. No integration test for the full pipeline
There's `benchmark/tests/integration/test_orchestrator.py` and `test_cli.py`, but no test that runs `generate_transcript` -> `orchestrator.score()` -> `ReportGenerator` end-to-end with a mocked API client. The actual data flow through the system is untested.

## Quick Wins

1. **Extract shared result-building logic in `runner.py`** — Create a `build_scenario_result(model, scenario, score_result) -> Dict` function used by all 4 code paths. Eliminates ~200 lines of duplication and the risk of result dict keys diverging between paths. (High impact, medium effort)

2. **Add `_parse_float_field()` and `_parse_bool_field()` helpers for LLM response parsing** — Replace the 20+ identical try/except blocks across scorers with a shared parser that logs warnings on parse failures and validates ranges. This catches scoring bugs from malformed LLM responses. (High impact, low effort)

3. **Validate `ScoringConfig` weights sum to 1.0** — Replace the no-op `validate_weights_sum` with a real `@model_validator(mode="after")` that checks `sum(self.weights.values())`. One-line fix that prevents silent scoring errors. (High impact, trivial effort)

4. **Reuse `httpx.AsyncClient` across calls** — Store the client as `self._async_client` on `ModelAPIClient`, create lazily on first async call, add a `close()` method. Reduces TLS overhead in parallel mode by ~90%. (Medium impact, low effort)

5. **Add mock-API scorer tests** — Create a `FakeAPIClient` that returns canned LLM responses, then test each scorer's parsing logic with representative LLM output (good response, partial response, malformed response). This covers the most critical untested code path. (High impact, medium effort)
